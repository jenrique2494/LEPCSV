# -*- coding: utf-8 -*-
"""CEFR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MnLOujn37-eI12amsBNrRGjW_xTar9W8
"""

# Instalar cefrpy
!pip install cefrpy

# Instalar spaCy
!pip install -U spacy

# Descargar el modelo de lenguaje en inglés para spaCy
!python -m spacy download en_core_web_sm

# Importar las clases necesarias de la biblioteca cefrpy
from cefrpy import CEFRAnalyzer, CEFRSpaCyAnalyzer
import heapq # Import the heapq module

# Importar spaCy y cargar el modelo de lenguaje en inglés
import spacy
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("Modelo 'en_core_web_sm' no encontrado.")
    print("Por favor, ejecute: python -m spacy download en_core_web_sm")
    exit()

def clasificar_palabra(palabra: str):
    """
    Clasifica una palabra individual en su nivel MCER.
    """
    print(f"--- Análisis de la palabra: '{palabra}' ---")
    analyzer = CEFRAnalyzer()
    nivel_promedio = analyzer.get_average_word_level_CEFR(palabra)

    if nivel_promedio:
        print(f"Nivel MCER Promedio: {nivel_promedio}")
    else:
        print("La palabra no fue encontrada en la base de datos de cefrpy.")
    print("-" * 30 + "\n")

def clasificar_frase(frase: str):
    """
    Clasifica cada palabra dentro de una frase y proporciona una
    estimación de nivel MCER general para todo el texto.
    """
    print(f"--- Análisis del texto ---")

    LEVEL_MAP = {
        1: "A1", 2: "A2", 3: "B1",
        4: "B2", 5: "C1", 6: "C2"
    }

    ABBREVIATION_MAP = {
        "'m": "am", "n't": "not", "'re": "are", "'s": "is",
        "'ve": "have", "'ll": "will", "'d": "would"
    }

    cefr_word_analyzer = CEFRAnalyzer()
    analyzer = CEFRSpaCyAnalyzer(cefr_word_analyzer, abbreviation_mapping=ABBREVIATION_MAP)

    doc = nlp(frase)
    resultado_analisis = analyzer.analize_doc(doc)

    palabras_no_encontradas = set()
    stats_counts = {}
    total_found_words = 0

    print("\n--- Detalle por Palabra ---")
    print(f"{'Palabra':<15} | {'Categoría (POS)':<15} | {'Nivel MCER':<10}")
    print("-" * 50)

    for token, token_info in zip(doc, resultado_analisis):
        palabra_texto = token_info[0]
        pos_tag = token_info[1]
        nivel_numerico = token_info[3]

        if nivel_numerico is None:
            nivel_etiqueta = "No Encontrado"
            if not token.is_punct:
                palabras_no_encontradas.add(palabra_texto)
        else:
            nivel_mapeado = round(nivel_numerico)
            nivel_etiqueta = LEVEL_MAP.get(nivel_mapeado, f"Nivel~{nivel_mapeado}")
            stats_counts[nivel_mapeado] = stats_counts.get(nivel_mapeado, 0) + 1
            total_found_words += 1

        # Opcional: Descomentar la siguiente línea si quieres ver el detalle de cada palabra
        # print(f"{palabra_texto:<15} | {pos_tag:<15} | {nivel_etiqueta:<10}")

    # --- CÁLCULO Y VISUALIZACIÓN DE LA CLASIFICACIÓN ÚNICA ---

    weighted_sum = 0

    print("\n--- Resumen Estadístico del Vocabulario ---")
    if stats_counts:
        for nivel_num, count in sorted(stats_counts.items()):
            percentage = (count / total_found_words) * 100 if total_found_words > 0 else 0
            nivel_tag = LEVEL_MAP.get(nivel_num, f"Nivel {nivel_num}")
            print(f"  - {nivel_tag}: {count} palabra(s) ({percentage:.1f}%)")
            # Acumular para el promedio ponderado
            weighted_sum += nivel_num * count
    else:
        print("No se encontraron suficientes palabras para generar estadísticas.")

    # Calcular y mostrar la clasificación general final
    if total_found_words > 0:
        average_level_num = weighted_sum / total_found_words
        final_level_rounded = round(average_level_num)
        final_level_tag = LEVEL_MAP.get(final_level_rounded, "Desconocido")

        print("\n==================================================")
        print(f"  NIVEL GENERAL ESTIMADO (por léxico): {final_level_tag}")
        print(f"  (Puntaje numérico promedio: {average_level_num:.2f})")
        print("==================================================")

    if palabras_no_encontradas:
        print("\n--- Palabras No Encontradas en el Léxico ---")
        print(f"  {', '.join(sorted(list(palabras_no_encontradas)))}")

    print("\n" + "="*55 + "\n")


def clasificar_frase_con_anclaje_dominante(frase: str, classifier):
    """
    Clasifica una frase utilizando la lógica de "Ancla Dominante".
    La dominancia (léxica o gramatical) se determina por el nivel más alto.
    """
    print(f"--- Análisis con Anclaje Dominante: '{frase}' ---")

    LEVEL_TO_NUM = {"A1": 1, "A2": 2, "B1": 3, "B2": 4, "C1": 5, "C2": 6}
    NUM_TO_LEVEL = {v: k for k, v in LEVEL_TO_NUM.items()}

    # --- 1. Análisis Léxico para encontrar la palabra de MÁS alto nivel ---
    cefr_word_analyzer = CEFRAnalyzer()
    analyzer = CEFRSpaCyAnalyzer(cefr_word_analyzer)
    doc = nlp(frase)
    resultado_analisis = analyzer.analize_doc(doc)

    lexical_anchor_num = 0
    for token_info in resultado_analisis:
        nivel_numerico = token_info[3]
        if nivel_numerico is not None and nivel_numerico > lexical_anchor_num:
            lexical_anchor_num = nivel_numerico

    if lexical_anchor_num == 0:
        print("No se encontraron palabras con nivel MCER para analizar.")
        # Fallback: If no lexical anchor found, assume minimum A1
        lexical_anchor_tag = "N/A"
        lexical_anchor_num = 1
    else:
         lexical_anchor_tag = NUM_TO_LEVEL.get(round(lexical_anchor_num), "N/A")


    print(f"Ancla Léxica Dominante (Palabra más difícil): {lexical_anchor_tag} ({lexical_anchor_num:.2f})")

    # --- 2. Obtener la "Pista Gramatical" usando el clasificador ---
    import json
    grammatical_scores = json.loads(classifier.predict(frase))
    top_grammatical_level_tag = max(grammatical_scores, key=grammatical_scores.get)
    grammatical_hint_num = LEVEL_TO_NUM.get(top_grammatical_level_tag, 0)
    print(f"Pista Gramatical (Análisis de estructura): {top_grammatical_level_tag} ({grammatical_hint_num:.2f})")

    # --- 3. Combinar con Ponderación Condicional ---
    # Define weights based on which level is higher
    if lexical_anchor_num >= grammatical_hint_num:
        # Lexical is dominant or equal
        lexical_weight = 0.9
        grammatical_weight = 0.1
        print("  -> Léxico es dominante.")
    else:
        # Grammatical is dominant
        lexical_weight = 0.1
        grammatical_weight = 0.9
        print("  -> Gramática es dominante.")


    final_score = (lexical_anchor_num * lexical_weight) + (grammatical_hint_num * grammatical_weight)

    final_level_num = round(final_score)
    final_level_num = max(1, final_level_num) # Asegurar que el mínimo sea A1
    final_level_tag = NUM_TO_LEVEL.get(final_level_num, "Indeterminado")

    print("\n--------------------------------------------------")
    print(f"  NIVEL FINAL ESTIMADO: {final_level_tag}")
    print(f"  (Puntaje final ponderado: {final_score:.2f})")
    print("--------------------------------------------------\n")


# ---------------------------------------------------------------------------
# EJEMPLOS DE USO DEL PROGRAMA
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    clasificar_palabra("happy")
    clasificar_palabra("supremacy")
    clasificar_palabra("degree")

    texto_largo = """get your bearings"""
    clasificar_frase(texto_largo)

    # Make sure the classifier is instantiated before calling the function
    # Assuming classifier is loaded in a previous cell or globally available
    # from google.colab import drive
    # import os
    # drive.mount('/content/drive') # Ensure drive is mounted

    # from transformers import AutoTokenizer, AutoModelForSequenceClassification
    # import torch

    # class CEFRClassifier:
    #     def __init__(self, model_path):
    #         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    #         self.tokenizer = AutoTokenizer.from_pretrained(model_path)
    #         self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
    #         self.model.to(self.device)
    #         self.model.eval()
    #     def predict(self, sentence: str) -> str:
    #         import json
    #         inputs = self.tokenizer(sentence, return_tensors="pt", padding=True, truncation=True)
    #         inputs = {k: v.to(self.device) for k, v in inputs.items()}
    #         with torch.no_grad():
    #             outputs = self.model(**inputs)
    #         logits = outputs.logits
    #         probabilities = torch.nn.functional.softmax(logits, dim=-1).squeeze()
    #         prob_dict = { self.model.config.id2label[i]: prob.item() for i, prob in enumerate(probabilities)}
    #         return json.dumps(prob_dict, indent=4)

    # model_save_path = "/content/drive/MyDrive/anki/cefr_classifier_model_final"
    # classifier = CEFRClassifier(model_save_path)


    frase_ejemplo_usuario = "I have absolute, supremacy serendipitous." # 'absolute' y 'supremacy' son C1/C2

    # Now call the function passing only the sentence and the classifier object
    # Make sure 'classifier' object is defined in a previous cell
    if 'classifier' in globals():
        clasificar_frase_con_anclaje_dominante(frase_ejemplo_usuario, classifier)
    else:
        print("Error: 'classifier' object not found. Please run the cell that loads the model.")

    # Example to test the new logic (assuming classifier is loaded)
    if 'classifier' in globals():
        print("\n--- Testing adjusted logic ---")
        low_lexical_high_grammar_sentence = "The cat sat on the mat and enjoy it so much." # Simple words, grammar might be A1/A2 but structure is simple
        # Simulate a high grammatical score from the classifier for this simple sentence
        # In a real scenario, the classifier would predict this.
        # For demonstration, let's use a simple sentence and see the lexical score first.
        print(f"\nTesting sentence: '{low_lexical_high_grammar_sentence}'")
        # The function will now handle getting the grammatical score internally
        clasificar_frase_con_anclaje_dominante(low_lexical_high_grammar_sentence, classifier)

# ==============================================================================
# SECCIÓN 1: INSTALACIÓN DE BIBLIOTECAS
# ==============================================================================
# Instalamos las bibliotecas necesarias de Hugging Face y scikit-learn en modo silencioso.
# - transformers: para cargar y usar modelos de lenguaje preentrenados.
# - datasets: para cargar y manipular el corpus de datos.
# - evaluate: para calcular métricas de rendimiento como la precisión.
# - accelerate: para optimizar el entrenamiento en PyTorch.
!pip install transformers datasets evaluate accelerate scikit-learn -q

# ==============================================================================
# SECCIÓN 2: IMPORTACIONES Y CONFIGURACIÓN INICIAL
# ==============================================================================
import pandas as pd
import numpy as np
import json
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
import evaluate
from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# ==============================================================================
# SECCIÓN 3: CARGA Y PREPARACIÓN DE DATOS
# ==============================================================================
print("--- Cargando y preparando el conjunto de datos ---")

# Cargar el dataset desde Hugging Face
try:
    dataset_hf = Dataset.load_from_disk("edesaras/CEFR-Sentence-Level-Annotations")
except Exception:
    from datasets import load_dataset
    dataset_hf = load_dataset("edesaras/CEFR-Sentence-Level-Annotations")


# Convertir a un DataFrame de pandas para facilitar la manipulación
df = pd.DataFrame(dataset_hf['train'])

# Consolidar las etiquetas de los dos anotadores promediando y redondeando
df['cefr_level_num'] = df[['Annotator I', 'Annotator II']].mean(axis=1).round().astype(int)

# Mapear los niveles numéricos (1-6) a las etiquetas del MCER (A1-C2)
cefr_map = {1: 'A1', 2: 'A2', 3: 'B1', 4: 'B2', 5: 'C1', 6: 'C2'}
df['label_str'] = df['cefr_level_num'].map(cefr_map)

# Crear un DataFrame limpio con las columnas necesarias
df_processed = df[['text', 'label_str']].copy()
df_processed.dropna(subset=['text', 'label_str'], inplace=True)

# Mapear las etiquetas de texto a números para el entrenamiento del modelo
labels = sorted(df_processed['label_str'].unique())
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for i, label in enumerate(labels)}
df_processed['label'] = df_processed['label_str'].map(label2id)

# Dividir los datos en conjuntos de entrenamiento, validación y prueba
train_val_df, test_df = train_test_split(
    df_processed,
    test_size=0.15,
    random_state=42,
    stratify=df_processed['label_str']
)

train_df, val_df = train_test_split(
    train_val_df,
    test_size=0.15,
    random_state=42,
    stratify=train_val_df['label_str']
)

# Convertir los DataFrames de pandas de nuevo a objetos Dataset de Hugging Face
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))

# Combinarlos en un DatasetDict
raw_datasets = DatasetDict({
    'train': train_dataset,
    'validation': val_dataset,
    'test': test_dataset
})

print("Datos cargados y preparados:")
print(raw_datasets)

# ==============================================================================
# SECCIÓN 4: TOKENIZACIÓN
# ==============================================================================
print("\n--- Tokenizando el texto ---")

# Elegir un modelo preentrenado. 'distilbert-base-uncased' es un buen equilibrio
# entre rendimiento y eficiencia computacional, ideal para Colab.
model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Función para tokenizar los textos
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Aplicar la tokenización a todo el conjunto de datos
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# Eliminar columnas innecesarias para el entrenamiento
tokenized_datasets = tokenized_datasets.remove_columns(["text", "label_str"])
tokenized_datasets.set_format("torch")

print("Tokenización completa.")

# ==============================================================================
# SECCIÓN 5: ENTRENAMIENTO DEL MODELO
# ==============================================================================
print("\n--- Configurando el entrenamiento del modelo ---")

# Cargar el modelo preentrenado, configurándolo para 6 etiquetas de clasificación
model = AutoModelForSequenceClassification.from_pretrained(
    model_checkpoint,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id
)

# Definir los argumentos de entrenamiento
# Estos hiperparámetros controlan el proceso de fine-tuning
training_args = TrainingArguments(
    output_dir="cefr_classifier_results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
)

# Definir la métrica para la evaluación
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Crear el objeto Trainer que gestionará el entrenamiento
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

print("\n--- ¡Comenzando el entrenamiento! (Esto puede tardar varios minutos) ---")
trainer.train()
print("--- Entrenamiento finalizado ---")

# Guardar el modelo final y el tokenizador
model_save_path = "/content/drive/MyDrive/anki/cefr_classifier_model_final"
# Create the directory if it doesn't exist
os.makedirs(model_save_path, exist_ok=True)

trainer.save_model(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"Modelo guardado en: {model_save_path}")

import pandas as pd
import numpy as np
import json
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
import evaluate
from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

clasifier = AutoModelForSequenceClassification.from_pretrained(
    "/content/drive/MyDrive/anki/cefr_classifier_model_final"
)


def obtener_nivel_mcerr(texto):
    """
    Clasifica una frase utilizando la lógica de "Ancla Dominante".
    La palabra de más alto nivel tiene un peso del 85% en el resultado final.
    """
    print(f"--- Análisis con Anclaje Dominante: '{texto}' ---")

    grammatical_scores_dict = json.loads(classifier.predict(texto))
    clasificar_frase_con_anclaje_dominante(texto, grammatical_scores_dict)
    return clasificar_frase_con_anclaje_dominante

import pandas as pd
import numpy as np
import json
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# ==============================================================================
# SECCIÓN 6: CLASE PARA INFERENCIA Y PREDICCIÓN
# ==============================================================================
print("\n--- Creando la clase para predicciones ---")

class CEFRClassifier:
    def __init__(self, model_path):
        """
        Inicializa el clasificador cargando el modelo y el tokenizador desde una ruta.
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval() # Poner el modelo en modo de evaluación
        print(f"Clasificador cargado y listo para usar en el dispositivo: {self.device}")

    def predict(self, sentence: str) -> str:
        """
        Predice las probabilidades del nivel MCER para una oración y devuelve un JSON.
        """
        # Tokenizar la oración de entrada
        inputs = self.tokenizer(sentence, return_tensors="pt", padding=True, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # Realizar la inferencia sin calcular gradientes para mayor eficiencia
        with torch.no_grad():
            outputs = self.model(**inputs)

        # Los 'logits' son las puntuaciones brutas del modelo para cada clase
        logits = outputs.logits

        # Aplicar la función softmax para convertir los logits en una distribución de probabilidad
        probabilities = torch.nn.functional.softmax(logits, dim=-1).squeeze()

        # Crear el diccionario de salida con las probabilidades para cada nivel
        prob_dict = {
            self.model.config.id2label[i]: prob.item()
            for i, prob in enumerate(probabilities)
        }

        # Devolver el resultado como una cadena JSON formateada
        return json.dumps(prob_dict, indent=4)

# Instantiate the classifier, loading the model from Drive
model_save_path = "/content/drive/MyDrive/anki/cefr_classifier_model_final"
# Ensure the directory exists before attempting to load
if os.path.exists(model_save_path):
    classifier = CEFRClassifier(model_save_path)
else:
    print(f"Error: Model directory not found at {model_save_path}")
    classifier = None # Set classifier to None if loading fails

# ==============================================================================
# Define the function that uses the classifier
# Assuming clasificar_frase_con_anclaje_dominante is defined elsewhere
# ==============================================================================
def obtener_nivel_mcerr(texto):
    """
    Clasifica una frase utilizando la lógica de "Ancla Dominante".
    It uses the pre-loaded classifier for grammatical scores.
    """
    if classifier is None:
        print("Classifier not loaded. Cannot perform classification.")
        return None

    print(f"--- Análisis con Anclaje Dominante: '{texto}' ---")

    # Use the pre-loaded classifier to get grammatical scores
    # grammatical_scores_dict = json.loads(classifier.predict(texto)) # This line is not needed here

    # Assuming clasificar_frase_con_anclaje_dominante is defined and accessible
    # This function needs to be defined in a previous cell
    try:
        # Pass the classifier object itself to clasificar_frase_con_anclaje_dominante
        clasificar_frase_con_anclaje_dominante(texto, classifier)
    except NameError:
        print("Error: 'clasificar_frase_con_anclaje_dominante' function not found.")
        print("Please ensure the cell containing the definition of 'clasificar_frase_con_anclaje_dominante' has been executed.")
        return None

    # The original function returned the function itself, which seems incorrect.
    # It should likely just perform the classification and print the result.
    # If a return value is needed, it would likely be the final level or score.
    # Returning None for now, or you can adjust based on desired output.
    return None # Or return the calculated level if needed

import json # Import the json library

if __name__ == "__main__":
    clasificar_palabra("happy")
    clasificar_palabra("supremacy")
    clasificar_palabra("degree")

    texto_largo = """get your bearings"""
    clasificar_frase(texto_largo)


    frase_ejemplo_usuario = "I" # 'absolute' y 'supremacy' son C1/C2

    obtener_nivel_mcerr("""my cousin sister is a high temperaperson that is widely happy for the exam of school and courses from english .""")