{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc8aace",
   "metadata": {},
   "source": [
    "# üéØ Clasificador CEFR con Modelo Personalizado\n",
    "## An√°lisis de Palabras y Frases usando tu Modelo Entrenado\n",
    "\n",
    "Este notebook:\n",
    "- üîó Se conecta a tu Google Drive\n",
    "- ü§ñ Usa tu modelo entrenado `cefr_classifier_model_final`\n",
    "- üìö Combina an√°lisis l√©xico (cefrpy) y gramatical (transformer)\n",
    "- üìÑ Procesa archivos TSV completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0140252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Instalaci√≥n de dependencias\n",
    "!pip install cefrpy spacy transformers torch -q\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "\n",
    "print(\"‚úÖ Dependencias instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîó Conectar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verificar que el modelo existe\n",
    "import os\n",
    "model_path = \"/content/drive/MyDrive/anki/cefr_classifier_model_final\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"‚úÖ Modelo encontrado en: {model_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå Modelo no encontrado en: {model_path}\")\n",
    "    print(\"üí° Verifica la ruta en tu Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Cargar el analizador CEFR completo\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from cefrpy import CEFRAnalyzer, CEFRSpaCyAnalyzer\n",
    "import spacy\n",
    "\n",
    "# Mapeos de niveles\n",
    "LEVEL_MAP = {1: \"A1\", 2: \"A2\", 3: \"B1\", 4: \"B2\", 5: \"C1\", 6: \"C2\"}\n",
    "LEVEL_TO_NUM = {\"A1\": 1, \"A2\": 2, \"B1\": 3, \"B2\": 4, \"C1\": 5, \"C2\": 6}\n",
    "\n",
    "class CEFRAnalyzer_Complete:\n",
    "    def __init__(self, model_path):\n",
    "        # Cargar modelo neural\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Cargar spaCy y cefrpy\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.cefrpy_analyzer = CEFRAnalyzer()\n",
    "        \n",
    "        print(f\"‚úÖ Analizador completo cargado en {self.device}\")\n",
    "    \n",
    "    def predict_grammar(self, sentence):\n",
    "        \"\"\"An√°lisis gramatical con modelo neural\"\"\"\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1).squeeze()\n",
    "        \n",
    "        prob_dict = {\n",
    "            self.model.config.id2label[i]: prob.item()\n",
    "            for i, prob in enumerate(probabilities)\n",
    "        }\n",
    "        \n",
    "        return prob_dict\n",
    "    \n",
    "    def analyze_lexical(self, text):\n",
    "        \"\"\"An√°lisis l√©xico detallado\"\"\"\n",
    "        spacy_analyzer = CEFRSpaCyAnalyzer(self.cefrpy_analyzer)\n",
    "        doc = self.nlp(text)\n",
    "        analysis = spacy_analyzer.analize_doc(doc)\n",
    "        \n",
    "        levels_found = []\n",
    "        words_analyzed = []\n",
    "        words_not_found = []\n",
    "        \n",
    "        for token, info in zip(doc, analysis):\n",
    "            if token.is_punct or token.is_space:\n",
    "                continue\n",
    "            \n",
    "            word = info[0]\n",
    "            level_num = info[3]\n",
    "            \n",
    "            if level_num is not None:\n",
    "                level_cefr = LEVEL_MAP.get(round(level_num), f\"N{level_num}\")\n",
    "                levels_found.append(level_num)\n",
    "                words_analyzed.append((word, level_cefr, level_num))\n",
    "            else:\n",
    "                words_not_found.append(word)\n",
    "        \n",
    "        result = {\n",
    "            'words_analyzed': words_analyzed,\n",
    "            'words_not_found': words_not_found,\n",
    "            'average_level': sum(levels_found) / len(levels_found) if levels_found else 1,\n",
    "            'max_level': max(levels_found) if levels_found else 1,\n",
    "            'levels_found': levels_found\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def classify_text(self, text, show_details=True):\n",
    "        \"\"\"Clasificaci√≥n completa con ancla dominante\"\"\"\n",
    "        if show_details:\n",
    "            print(f\"üîç Analizando: '{text}'\")\n",
    "        \n",
    "        # 1. An√°lisis l√©xico\n",
    "        lexical = self.analyze_lexical(text)\n",
    "        \n",
    "        # 2. An√°lisis gramatical\n",
    "        grammatical = self.predict_grammar(text)\n",
    "        \n",
    "        # Encontrar nivel gramatical top\n",
    "        gram_top_level = max(grammatical, key=grammatical.get)\n",
    "        gram_confidence = grammatical[gram_top_level]\n",
    "        gram_level_num = LEVEL_TO_NUM.get(gram_top_level, 1)\n",
    "        \n",
    "        # 3. Aplicar l√≥gica de ancla dominante\n",
    "        lex_max = lexical['max_level']\n",
    "        lex_avg = lexical['average_level']\n",
    "        \n",
    "        if show_details:\n",
    "            print(f\"üìö L√©xico - M√°ximo: {LEVEL_MAP.get(round(lex_max), 'A1')} | Promedio: {lex_avg:.2f}\")\n",
    "            print(f\"ü§ñ Gram√°tica: {gram_top_level} (confianza: {gram_confidence:.3f})\")\n",
    "            \n",
    "            # Mostrar palabras analizadas\n",
    "            if lexical['words_analyzed']:\n",
    "                print(\"\\nüìä Palabras analizadas:\")\n",
    "                for word, level, score in lexical['words_analyzed'][:10]:  # Mostrar m√°ximo 10\n",
    "                    print(f\"  {word:<15} ‚Üí {level}\")\n",
    "                if len(lexical['words_analyzed']) > 10:\n",
    "                    print(f\"  ... y {len(lexical['words_analyzed']) - 10} m√°s\")\n",
    "        \n",
    "        # Determinar dominancia\n",
    "        if lex_max >= gram_level_num:\n",
    "            # L√©xico domina\n",
    "            lexical_weight = 0.85\n",
    "            grammatical_weight = 0.15\n",
    "            dominance = \"lexical\"\n",
    "        else:\n",
    "            # Gram√°tica domina\n",
    "            if gram_confidence > 0.7:\n",
    "                lexical_weight = 0.25\n",
    "                grammatical_weight = 0.75\n",
    "            else:\n",
    "                lexical_weight = 0.4\n",
    "                grammatical_weight = 0.6\n",
    "            dominance = \"grammatical\"\n",
    "        \n",
    "        # Calcular nivel final\n",
    "        final_score = (lex_max * lexical_weight) + (gram_level_num * grammatical_weight)\n",
    "        final_level = LEVEL_MAP.get(round(final_score), 'A1')\n",
    "        \n",
    "        if show_details:\n",
    "            print(f\"\\nüéØ Dominancia: {dominance}\")\n",
    "            print(f\"‚úÖ NIVEL FINAL: {final_level} (puntaje: {final_score:.2f})\")\n",
    "            \n",
    "            if lexical['words_not_found']:\n",
    "                print(f\"‚ö†Ô∏è  Palabras no encontradas: {', '.join(lexical['words_not_found'][:5])}\")\n",
    "        \n",
    "        return {\n",
    "            'final_level': final_level,\n",
    "            'final_score': final_score,\n",
    "            'lexical_max': lex_max,\n",
    "            'grammatical_level': gram_top_level,\n",
    "            'grammatical_confidence': gram_confidence,\n",
    "            'dominance': dominance,\n",
    "            'lexical_details': lexical,\n",
    "            'grammatical_details': grammatical\n",
    "        }\n",
    "    \n",
    "    def classify_word(self, word, show_details=True):\n",
    "        \"\"\"Clasificaci√≥n de palabra individual\"\"\"\n",
    "        if show_details:\n",
    "            print(f\"üîç Analizando palabra: '{word}'\")\n",
    "        \n",
    "        try:\n",
    "            level = self.cefrpy_analyzer.get_average_word_level_CEFR(word)\n",
    "            if level:\n",
    "                level_rounded = round(level)\n",
    "                level_cefr = LEVEL_MAP.get(level_rounded, f\"Nivel {level}\")\n",
    "                if show_details:\n",
    "                    print(f\"‚úÖ Nivel CEFR: {level_cefr} (puntuaci√≥n: {level:.2f})\")\n",
    "                return level_cefr\n",
    "            else:\n",
    "                if show_details:\n",
    "                    print(\"‚ùå Palabra no encontrada en base de datos\")\n",
    "                # Heur√≠stica simple\n",
    "                if len(word) <= 4:\n",
    "                    return 'A1'\n",
    "                elif len(word) <= 6:\n",
    "                    return 'A2'\n",
    "                elif len(word) <= 8:\n",
    "                    return 'B1'\n",
    "                else:\n",
    "                    return 'B2'\n",
    "        except Exception as e:\n",
    "            if show_details:\n",
    "                print(f\"‚ö†Ô∏è  Error: {e}\")\n",
    "            return 'A1'\n",
    "\n",
    "# Inicializar analizador\n",
    "try:\n",
    "    analyzer = CEFRAnalyzer_Complete(model_path)\n",
    "    print(\"üéâ ¬°Analizador listo para usar!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error inicializando: {e}\")\n",
    "    analyzer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Pruebas de ejemplo\n",
    "if analyzer:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üß™ PRUEBAS DE EJEMPLO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Probar palabras individuales\n",
    "    test_words = [\"happy\", \"supremacy\", \"serendipitous\", \"cat\", \"democracy\"]\n",
    "    \n",
    "    print(\"\\nüìù PALABRAS INDIVIDUALES:\")\n",
    "    for word in test_words:\n",
    "        result = analyzer.classify_word(word, show_details=False)\n",
    "        print(f\"  {word:<15} ‚Üí {result}\")\n",
    "    \n",
    "    # Probar frases\n",
    "    test_sentences = [\n",
    "        \"I am happy\",\n",
    "        \"The weather is beautiful today\",\n",
    "        \"Democracy requires active participation from citizens\",\n",
    "        \"The serendipitous discovery revolutionized the field\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\\nüìñ FRASES COMPLETAS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        print(f\"\\n{'‚îÄ' * 40}\")\n",
    "        result = analyzer.classify_text(sentence)\n",
    "        print(f\"\")\n",
    "else:\n",
    "    print(\"‚ùå No se puede ejecutar pruebas sin el analizador\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí¨ An√°lisis interactivo\n",
    "if analyzer:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üí¨ AN√ÅLISIS INTERACTIVO\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Ingresa palabras o frases para analizar\")\n",
    "    print(\"Escribe 'exit' para terminar\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            text = input(\"\\nüéØ Texto a analizar: \").strip()\n",
    "            \n",
    "            if text.lower() == 'exit':\n",
    "                print(\"üëã ¬°Hasta luego!\")\n",
    "                break\n",
    "            \n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            print(\"\\n\" + \"‚îÄ\" * 50)\n",
    "            \n",
    "            if ' ' in text:\n",
    "                # Es una frase\n",
    "                result = analyzer.classify_text(text)\n",
    "            else:\n",
    "                # Es una palabra\n",
    "                level = analyzer.classify_word(text)\n",
    "                print(f\"üéØ Resultado: {text} ‚Üí {level}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã ¬°Hasta luego!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Analizador no disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd779bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÑ Procesamiento de archivo TSV\n",
    "def process_tsv_file(file_path, analyzer, max_entries=None):\n",
    "    \"\"\"Procesa archivo TSV y agrega niveles CEFR\"\"\"\n",
    "    print(f\"üìÅ Procesando: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        processed_lines = []\n",
    "        processed_count = 0\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.rstrip('\\n\\r')\n",
    "            \n",
    "            # Procesar l√≠neas que no sean comentarios\n",
    "            if line.startswith('#') or not line.strip():\n",
    "                processed_lines.append(line)\n",
    "                continue\n",
    "            \n",
    "            columns = line.split('\\t')\n",
    "            \n",
    "            if len(columns) >= 12:\n",
    "                word_phrase = columns[3].strip()\n",
    "                \n",
    "                if word_phrase and (max_entries is None or processed_count < max_entries):\n",
    "                    print(f\"\\nüîÑ Procesando {processed_count + 1}: {word_phrase}\")\n",
    "                    \n",
    "                    # Determinar si es palabra o frase\n",
    "                    if ' ' in word_phrase:\n",
    "                        result = analyzer.classify_text(word_phrase, show_details=True)\n",
    "                        cefr_level = result['final_level']\n",
    "                    else:\n",
    "                        cefr_level = analyzer.classify_word(word_phrase, show_details=True)\n",
    "                    \n",
    "                    # Agregar a columna 12\n",
    "                    if columns[11].strip():\n",
    "                        columns[11] = columns[11] + ' ' + cefr_level\n",
    "                    else:\n",
    "                        columns[11] = cefr_level\n",
    "                    \n",
    "                    processed_count += 1\n",
    "                    print(f\"‚úÖ ‚Üí {cefr_level}\")\n",
    "                \n",
    "                processed_lines.append('\\t'.join(columns))\n",
    "            else:\n",
    "                processed_lines.append(line)\n",
    "        \n",
    "        # Guardar archivo procesado\n",
    "        output_path = file_path.replace('.txt', '_CEFR_completo.txt')\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for line in processed_lines:\n",
    "                f.write(line + '\\n')\n",
    "        \n",
    "        print(f\"\\nüéâ ¬°Completado!\")\n",
    "        print(f\"üìä Entradas procesadas: {processed_count}\")\n",
    "        print(f\"üíæ Guardado como: {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error procesando archivo: {e}\")\n",
    "        return None\n",
    "\n",
    "# Para procesar tu archivo, primero s√∫belo a Colab y luego ejecuta:\n",
    "if analyzer:\n",
    "    print(\"üìã Para procesar tu archivo TSV:\")\n",
    "    print(\"1. Sube el archivo a Colab usando el panel de archivos\")\n",
    "    print(\"2. Ejecuta: process_tsv_file('nombre_archivo.txt', analyzer)\")\n",
    "    print(\"\\nüí° Ejemplo:\")\n",
    "    print(\"# process_tsv_file('/content/4000EEnglish__1.Book copy.txt', analyzer, max_entries=10)\")\n",
    "    print(\"\\n‚ö†Ô∏è  Usa max_entries para limitar el procesamiento en pruebas\")\n",
    "else:\n",
    "    print(\"‚ùå Analizador no disponible para procesar archivos\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
